{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from collections import Counter\n",
    "from keras import layers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library = \"C:\\\\Users\\\\liblabs-user\\\\Desktop\\\\song-authorship\\\\data\"\n",
    "laptop = \"not yet\"\n",
    "desktop = \"C:\\\\Users\\\\Sam\\\\Desktop\\\\song authorship\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lyrics do not have sentences, we will delimit the sentences using the new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text, names):\n",
    "    remove = [\"_\", \"*\", \"--\", \",\", \"!\", \"?\"]\n",
    "    repl = {\"\\<u\\+0092\\>\": \"\\'\"}\n",
    "    text = text.lower()\n",
    "    for x in remove:\n",
    "        text = text.replace(x, \"\")\n",
    "    for x in repl:\n",
    "        text = re.sub(x, repl[x], text)\n",
    "    return [x.strip() for x in re.split(\"\\n\", text) if x != \"\"]\n",
    "\n",
    "def data_generator(robot, nonrobot, s_min, s_max, a_min, a_max,\n",
    "                   lookback, batch_size=128):\n",
    "\n",
    "    if s_max is None:\n",
    "        s_max = len(robot)\n",
    "    if a_max is None:\n",
    "        a_max = len(nonrobot)\n",
    "\n",
    "    si = s_min\n",
    "    ai = a_min\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        follow = np.random.randint(0, 2, batch_size)\n",
    "        sind = []\n",
    "        aind = []\n",
    "\n",
    "        for x in follow:\n",
    "            # robotic is 0\n",
    "            if x == 0:\n",
    "                if si >= s_max:\n",
    "                    si = s_min\n",
    "                sind.append(si)\n",
    "                aind.append(-1)\n",
    "                si += 1\n",
    "            else:\n",
    "                if ai >= a_max:\n",
    "                    ai = a_min\n",
    "                aind.append(ai)\n",
    "                sind.append(-1)\n",
    "                ai += 1\n",
    "\n",
    "        samples = []\n",
    "\n",
    "        # now pad sentences and yield.\n",
    "        for x in range(len(sind)):\n",
    "            # sampled non-robotic here\n",
    "            if sind[x] == -1:\n",
    "                samples.append(nonrobot[aind[x]])\n",
    "            else:\n",
    "                samples.append(robot[sind[x]])\n",
    "\n",
    "        yield np.array(samples), follow\n",
    "        \n",
    "def combine_phrases(n, song):\n",
    "    \"\"\"\n",
    "    n: Number of phrases to combine together\n",
    "    song: list of phrases to combine\n",
    "    \n",
    "    returns: list of combined sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    add = []\n",
    "    for i in range(0, len(song) - n, n):\n",
    "        add.append(\" \".join(song[i: i + n]))\n",
    "    add.append(\" \".join(song[i + n:]))\n",
    "    print(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"walking on air tonight tonight tonight i'm walking on air tonight tonight tonight i'm walking on air\", \"you're giving me sweet sweet ecstasy yeah you take me to utopia you're reading me like erotica\", \"boy you make me feel exotic yeah just when i think i can't take anymore we go deeper and hotter than ever before\", \"we go higher and higher i feel like i'm already there i'm walking on air tonight\", \"i'm walking on air i'm walking i'm walking on air tonight i'm walking on air\", \"i'm walking on air tonight i'm walking on air i'm walking i'm walking on air tonight\", \"i'm walking on air i'm walking on air this is pure paradise\", 'even heaven is jealous of our love yes we make angels cry raining down on us from up above', \"just when i think i can't take anymore we go deeper and hotter than ever before we go higher and higher\", \"i feel like i'm already there i'm walking on air tonight i'm walking on air\", \"i'm walking i'm walking on air tonight i'm walking on air i'm walking on air tonight\", \"i'm walking on air i'm walking i'm walking on air tonight i'm walking on air\", \"i'm walking on air tonight tonight tonight i'm walking on air tonight tonight tonight i'm walking on air\", \"heaven is jealous of our love angels are crying from up above i'm walking on air tonight\", \"i'm walking on air i'm walking i'm walking on air tonight i'm walking on air\", \"i'm walking on air tonight i'm walking on air i'm walking i'm walking on air tonight\", \"i'm walking on air i'm walking on air tonight tonight tonight i'm walking on air\", \"tonight tonight tonight i'm walking on air i'm walking on air\"]\n"
     ]
    }
   ],
   "source": [
    "combine_phrases(3, robotic[214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "number_of_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(desktop + \"\\\\Weekly_data_top_week.csv\")\n",
    "#names = \"(?<![A-Z])(?<![A-Z][A-z])(?<![A-Z][A-z][A-z])\\.\"\n",
    "names = \"\\s\"\n",
    "\n",
    "robotic = [clean_text(x, names) for x in df[df.Songwriter == True].Lyrics]\n",
    "#for i in range(10):\n",
    "#    print(robotic[i])\n",
    "nonrobotic = [clean_text(x, names) for x in df[df.Songwriter == False].Lyrics]\n",
    "\n",
    "# test train split\n",
    "rt = np.random.choice(len(robotic),\n",
    "                      round(len(robotic) * split),\n",
    "                      replace=False)\n",
    "rtrain = [y.split(\" \") for x in rt for y in robotic[x]]\n",
    "#print(rtrain[0:10])\n",
    "rtest = [y.split(\" \") for x in set(range(len(robotic))).difference(rt) for y in robotic[x]]\n",
    "\n",
    "nt = np.random.choice(len(nonrobotic),\n",
    "                      round(len(nonrobotic) * split),\n",
    "                      replace=False)\n",
    "ntrain = [y.split(\" \") for x in nt for y in nonrobotic[x]]\n",
    "ntest = [y.split(\" \") for x in set(range(len(nonrobotic))).difference(nt) for y in nonrobotic[x]]\n",
    "\n",
    "# vectorize sentences\n",
    "tokenizer = Tokenizer(num_words=number_of_words,\n",
    "                      filters=\"'!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\",\n",
    "                      lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(rtrain)\n",
    "tokenizer.fit_on_texts(ntrain)\n",
    "tokenizer.fit_on_texts(rtest)\n",
    "tokenizer.fit_on_texts(ntest)\n",
    "\n",
    "rtrain = pad_sequences(tokenizer.texts_to_sequences(rtrain), 10)\n",
    "#print()\n",
    "#print(rtrain[0:10])\n",
    "rtest = pad_sequences(tokenizer.texts_to_sequences(rtest), 10)\n",
    "ntrain = pad_sequences(tokenizer.texts_to_sequences(ntrain), 10)\n",
    "ntest = pad_sequences(tokenizer.texts_to_sequences(ntest), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34027"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tokenizer.word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.6922 - acc: 0.5242 - val_loss: 0.6932 - val_acc: 0.5074\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50742, saving model to weights-improvement-01-0.5074.h5\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6880 - acc: 0.5469 - val_loss: 0.6963 - val_acc: 0.5202\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50742 to 0.52023, saving model to weights-improvement-02-0.5202.h5\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6737 - acc: 0.5859 - val_loss: 0.7099 - val_acc: 0.5043\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6871 - acc: 0.5597 - val_loss: 0.6866 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.52023 to 0.54602, saving model to weights-improvement-04-0.5460.h5\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6814 - acc: 0.5770 - val_loss: 0.6984 - val_acc: 0.5336\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6709 - acc: 0.5873 - val_loss: 0.7042 - val_acc: 0.5156\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6623 - acc: 0.6061 - val_loss: 0.6997 - val_acc: 0.5298\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.6707 - acc: 0.5821 - val_loss: 0.6863 - val_acc: 0.5532\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.54602 to 0.55320, saving model to weights-improvement-08-0.5532.h5\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6564 - acc: 0.6109 - val_loss: 0.7100 - val_acc: 0.5442\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 7s 35ms/step - loss: 0.6533 - acc: 0.6192 - val_loss: 0.6976 - val_acc: 0.5542\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.55320 to 0.55422, saving model to weights-improvement-10-0.5542.h5\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 7s 36ms/step - loss: 0.6512 - acc: 0.6213 - val_loss: 0.7077 - val_acc: 0.5238\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.6628 - acc: 0.5945 - val_loss: 0.7011 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 7s 37ms/step - loss: 0.6395 - acc: 0.6350 - val_loss: 0.7070 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/100\n",
      " 15/200 [=>............................] - ETA: 6s - loss: 0.6505 - acc: 0.6031"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-845a1022cde2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m                               \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                               validation_steps=200)\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;31m#with open(\"Embedding.dat\", \"w\") as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lookback = 10\n",
    "batch_size = 64\n",
    "\n",
    "#stoker = pad_sequences(np.array(json.load(open(sys.argv[1].strip()))), 60)\n",
    "#austen = pad_sequences(np.array(json.load(open(sys.argv[2].strip()))), 60)\n",
    "\n",
    "#stoker_train = stoker[:round(len(stoker) * 0.9)]\n",
    "#stoker_val = stoker[round(len(stoker) * 0.9):]\n",
    "\n",
    "#austen_train = austen[:round(len(austen) * 0.9)]\n",
    "#austen_val = austen[round(len(austen) * 0.9):]\n",
    "\n",
    "train_gen = data_generator(robot=rtrain,\n",
    "                           nonrobot=ntrain,\n",
    "                           s_min=0,\n",
    "                           s_max=len(rtrain),\n",
    "                           a_min=0,\n",
    "                           a_max=len(ntrain),\n",
    "                           lookback=lookback,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "val_gen = data_generator(robot=rtest,\n",
    "                         nonrobot=ntest,\n",
    "                         s_min=0,\n",
    "                         s_max=len(rtest),\n",
    "                         a_min=0,\n",
    "                         a_max=len(ntest),\n",
    "                         lookback=lookback,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(number_of_words, 128))\n",
    "model.add(layers.LSTM(16,\n",
    "                        activation=\"relu\",\n",
    "                        recurrent_dropout=0.1,\n",
    "                        return_sequences=True))\n",
    "model.add(layers.LSTM(16,\n",
    "                        activation=\"relu\",\n",
    "                        recurrent_dropout=0.1,\n",
    "                        return_sequences=True))\n",
    "model.add(layers.LSTM(16,\n",
    "                     activation=\"relu\",\n",
    "                     recurrent_dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=RMSprop(),\n",
    "              loss=\"binary_crossentropy\",\n",
    "                metrics=[\"acc\"])\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}-{val_acc:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=200,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=200)\n",
    "\n",
    "#with open(\"Embedding.dat\", \"w\") as f:\n",
    "#    porv = load_model(\"RoboticorNot.h5\")\n",
    "#    json.dump(list([list(x) for x in\n",
    "#                    porv.layers[0].get_weights()[0].astype(float)]), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = history.history\n",
    "plt.plot(h[\"acc\"], c = \"b\")\n",
    "plt.plot(h[\"val_acc\"], c = \"r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
