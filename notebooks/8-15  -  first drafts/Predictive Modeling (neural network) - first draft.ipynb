{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from keras import layers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = \"C:\\\\Users\\\\liblabs-user\\\\Desktop\\\\song-authorship\\\\data\"\n",
    "laptop = \"not yet\"\n",
    "desktop = \"C:\\\\Users\\\\Sam\\\\Desktop\\\\song authorship\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lyrics do not have sentences, we will delimit the sentences using the new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, names):\n",
    "    remove = [\"_\", \"*\", \"--\", \",\", \"!\"]\n",
    "    for x in remove:\n",
    "        text = text.replace(x, \"\")\n",
    "    return [x.strip().lower() for x in re.split(\"\\n\", text) if x != \"\"]\n",
    "\n",
    "def data_generator(robot, nonrobot, s_min, s_max, a_min, a_max,\n",
    "                   lookback, batch_size=128):\n",
    "\n",
    "    if s_max is None:\n",
    "        s_max = len(robot)\n",
    "    if a_max is None:\n",
    "        a_max = len(nonrobot)\n",
    "\n",
    "    si = s_min\n",
    "    ai = a_min\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        follow = np.random.randint(0, 2, batch_size)\n",
    "        sind = []\n",
    "        aind = []\n",
    "\n",
    "        for x in follow:\n",
    "            # robotic is 0\n",
    "            if x == 0:\n",
    "                if si >= s_max:\n",
    "                    si = s_min\n",
    "                sind.append(si)\n",
    "                aind.append(-1)\n",
    "                si += 1\n",
    "            else:\n",
    "                if ai >= a_max:\n",
    "                    ai = a_min\n",
    "                aind.append(ai)\n",
    "                sind.append(-1)\n",
    "                ai += 1\n",
    "\n",
    "        samples = []\n",
    "\n",
    "        # now pad sentences and yield.\n",
    "        for x in range(len(sind)):\n",
    "            # sampled non-robotic here\n",
    "            if sind[x] == -1:\n",
    "                samples.append(nonrobot[aind[x]])\n",
    "            else:\n",
    "                samples.append(robot[sind[x]])\n",
    "\n",
    "        yield np.array(samples), follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(library + \"\\\\Weekly_data_top_week.csv\")\n",
    "#names = \"(?<![A-Z])(?<![A-Z][A-z])(?<![A-Z][A-z][A-z])\\.\"\n",
    "names = \"\\s\"\n",
    "\n",
    "robotic = [clean_text(x, names) for x in df[df.Songwriter == True].Lyrics]\n",
    "nonrobotic = [clean_text(x, names) for x in df[df.Songwriter == False].Lyrics]\n",
    "\n",
    "# test train split\n",
    "rt = np.random.choice(len(robotic),\n",
    "                      round(len(robotic) * split),\n",
    "                      replace=False)\n",
    "rtrain = [y.split(\" \") for x in rt for y in robotic[x]]\n",
    "#print(rtrain[0])\n",
    "rtest = [y.split(\" \") for x in set(range(len(robotic))).difference(rt) for y in robotic[x]]\n",
    "\n",
    "nt = np.random.choice(len(nonrobotic),\n",
    "                      round(len(nonrobotic) * split),\n",
    "                      replace=False)\n",
    "ntrain = [y.split(\" \") for x in nt for y in nonrobotic[x]]\n",
    "ntest = [y.split(\" \") for x in set(range(len(nonrobotic))).difference(nt) for y in nonrobotic[x]]\n",
    "\n",
    "# vectorize sentences\n",
    "tokenizer = Tokenizer(num_words=5000,\n",
    "                      filters=\"'!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\",\n",
    "                      lower=True, split=' ')\n",
    "tokenizer.fit_on_texts(rtrain)\n",
    "tokenizer.fit_on_texts(ntrain)\n",
    "\n",
    "rtrain = pad_sequences(tokenizer.texts_to_sequences(rtrain), 10)\n",
    "#print(rtrain[0])\n",
    "rtest = pad_sequences(tokenizer.texts_to_sequences(rtest), 10)\n",
    "ntrain = pad_sequences(tokenizer.texts_to_sequences(ntrain), 10)\n",
    "ntest = pad_sequences(tokenizer.texts_to_sequences(ntest), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21476"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tokenizer.word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "200/200 [==============================] - 6s 28ms/step - loss: 0.6931 - acc: 0.5045 - val_loss: 0.6928 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52266, saving model to weights-improvement-01-0.5227.h5\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.6920 - acc: 0.5250 - val_loss: 0.6924 - val_acc: 0.5103\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.52266\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6830 - acc: 0.5604 - val_loss: 0.6937 - val_acc: 0.5168\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.52266\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6826 - acc: 0.5668 - val_loss: 0.6885 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.52266 to 0.53477, saving model to weights-improvement-04-0.5348.h5\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6694 - acc: 0.5971 - val_loss: 0.6994 - val_acc: 0.5476\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.53477 to 0.54758, saving model to weights-improvement-05-0.5476.h5\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6740 - acc: 0.5870 - val_loss: 0.7046 - val_acc: 0.5378\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.54758\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6675 - acc: 0.5970 - val_loss: 0.7079 - val_acc: 0.5577\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.54758 to 0.55766, saving model to weights-improvement-07-0.5577.h5\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6602 - acc: 0.6360 - val_loss: 0.7174 - val_acc: 0.5343\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.55766\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6462 - acc: 0.6314 - val_loss: 0.7406 - val_acc: 0.5507\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.55766\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6494 - acc: 0.6304 - val_loss: 0.7277 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.55766\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6405 - acc: 0.6457 - val_loss: 0.8238 - val_acc: 0.5308\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.55766\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6243 - acc: 0.6676 - val_loss: 0.7335 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.55766\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6354 - acc: 0.6432 - val_loss: 0.7276 - val_acc: 0.5491\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.55766\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6131 - acc: 0.6756 - val_loss: 0.8078 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.55766\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6203 - acc: 0.6648 - val_loss: 0.7686 - val_acc: 0.5430\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.55766\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.6103 - acc: 0.6712 - val_loss: 0.7861 - val_acc: 0.5382\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.55766\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 3s 17ms/step - loss: 0.6214 - acc: 0.6644 - val_loss: 0.8118 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.55766\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.6113 - acc: 0.6752 - val_loss: 0.7773 - val_acc: 0.5441\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.55766\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.6087 - acc: 0.6817 - val_loss: 0.7724 - val_acc: 0.5241\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.55766\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.5992 - acc: 0.6854 - val_loss: 0.7629 - val_acc: 0.5494\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.55766\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.6055 - acc: 0.6788 - val_loss: 0.7389 - val_acc: 0.5459\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.55766\n",
      "Epoch 22/50\n",
      "141/200 [====================>.........] - ETA: 0s - loss: 0.5925 - acc: 0.6893"
     ]
    }
   ],
   "source": [
    "lookback = 10\n",
    "batch_size = 64\n",
    "\n",
    "#stoker = pad_sequences(np.array(json.load(open(sys.argv[1].strip()))), 60)\n",
    "#austen = pad_sequences(np.array(json.load(open(sys.argv[2].strip()))), 60)\n",
    "\n",
    "#stoker_train = stoker[:round(len(stoker) * 0.9)]\n",
    "#stoker_val = stoker[round(len(stoker) * 0.9):]\n",
    "\n",
    "#austen_train = austen[:round(len(austen) * 0.9)]\n",
    "#austen_val = austen[round(len(austen) * 0.9):]\n",
    "\n",
    "train_gen = data_generator(robot=rtrain,\n",
    "                           nonrobot=ntrain,\n",
    "                           s_min=0,\n",
    "                           s_max=len(rtrain),\n",
    "                           a_min=0,\n",
    "                           a_max=len(ntrain),\n",
    "                           lookback=lookback,\n",
    "                           batch_size=batch_size)\n",
    "\n",
    "val_gen = data_generator(robot=rtest,\n",
    "                         nonrobot=ntest,\n",
    "                         s_min=0,\n",
    "                         s_max=len(rtest),\n",
    "                         a_min=0,\n",
    "                         a_max=len(ntest),\n",
    "                         lookback=lookback,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(5000, 16))\n",
    "model.add(layers.LSTM(16,\n",
    "                        activation=\"relu\",\n",
    "                        dropout=0.2,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True))\n",
    "model.add(layers.LSTM(16,\n",
    "                        activation=\"relu\",\n",
    "                        dropout=0.2,\n",
    "                        recurrent_dropout=0.2,\n",
    "                        return_sequences=True))\n",
    "model.add(layers.LSTM(16,\n",
    "                     activation=\"relu\",\n",
    "                     dropout=0.2,\n",
    "                     recurrent_dropout=0.2))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=RMSprop(),\n",
    "              loss=\"binary_crossentropy\",\n",
    "                metrics=[\"acc\"])\n",
    "\n",
    "filepath = \"weights-improvement-{epoch:02d}-{val_acc:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc',\n",
    "                             verbose=1, save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=200,\n",
    "                              epochs=50,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=200)\n",
    "\n",
    "#with open(\"Embedding.dat\", \"w\") as f:\n",
    "#    porv = load_model(\"RoboticorNot.h5\")\n",
    "#    json.dump(list([list(x) for x in\n",
    "#                    porv.layers[0].get_weights()[0].astype(float)]), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
