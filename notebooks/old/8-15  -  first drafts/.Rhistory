library(tm)
library(topicmodels)
library(broom)
library = "C:\\Users\\liblabs-user\\Desktop\\song-authorship\\data"
laptop = "not yet"
desktop = "C:\\Users\\Sam\\Desktop\\song authorship\\data"
setwd(desktop)
df <- read.csv(
paste(desktop, "\\Weekly_data_clean.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
df[is.na(df$Weeks.on.chart),"Weeks.on.chart"] <- 1
max_week <- df %>% group_by(Artists, Name) %>%
summarise(Weeks.on.chart = max(Weeks.on.chart, na.rm = TRUE))
#max_week[mapply(is.infinite, max_week)] <- 1
final_df <- merge(x = max_week, y = df, by = c("Artists", "Name", "Weeks.on.chart"), all.x = TRUE)
final_df$ID <- seq(1:nrow(final_df))
write.csv(final_df, file = "Weekly_data_top_week.csv")
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df_words <- final_df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
#df_words$word <- text_tokens(df_words$word, stemmer = "en")
unique_df_words <- df_words %>% distinct(Name, Artists, word, .keep_all = TRUE)
write.csv(df_words, file = "Weekly_data_tokenized.csv")
write.csv(unique_df_words, file = "Weekly_data_tokenized_unique.csv")
num_words_to_filter = 5
ratios <- unique_df_words %>%
count(word, Songwriter) %>%
filter(n > num_words_to_filter) %>%
spread(Songwriter, n, fill = 0) %>%
ungroup() %>% #what is this used for?
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(True / False)) %>%
arrange(desc(logratio))
rbind(ratios %>% top_n(30, logratio), ratios %>% top_n(-30, logratio)) %>%
mutate(word = reorder(word, logratio)) %>%
ggplot(aes(word, logratio)) +
geom_col(show.legend = FALSE) +
coord_flip()
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
sources <- df_words %>%
group_by(Songwriter) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(Name, Songwriter, total_words)
by_source_sentiment <- df_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, Name) %>%
ungroup() %>%
complete(sentiment, Name, fill = list(n = 0)) %>%
inner_join(sources, "Name") %>%
group_by(Songwriter, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
sentiment_diff <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words))) %>%
ungroup()
# plot
sentiment_diff %>%
mutate(sentiment = reorder(sentiment, estimate)) %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
# do we even want this?
ratios <- unique_df_words %>%
count(word, Songwriter) %>%
filter(n > num_words_to_filter) %>%
spread(Songwriter, n, fill = 0) %>%
ungroup() %>% #what is this used for?
mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
mutate(logratio = log2(True / False)) %>%
arrange(desc(logratio))
top_and_bottom <- ratios %>%
inner_join(nrc, by = "word") %>%
filter(!sentiment %in% c("positive", "negative")) %>%
mutate(sentiment = reorder(sentiment, -logratio),
word = reorder(word, -logratio)) %>%
group_by(sentiment)
#rbind(ratios %>% top_n(30, logratio), ratios %>% top_n(-30, logratio))
rbind(top_and_bottom %>% top_n(5, logratio), top_and_bottom %>% top_n(-5, logratio)) %>%
ungroup() %>%
ggplot(aes(word, logratio, fill = logratio < 0)) +
facet_wrap(~ sentiment, scales = "free", nrow = 2) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 50, hjust = 1)) +
labs(x = "", y = "Robotic / non-Robotic log ratio") +
scale_fill_manual(name = "", labels = c("Robotic", "non-Robotic"),
values = c("#E69F00", "#56B4E9"))
popdf <- unique_df_words %>%
filter(str_detect(Genre, regex("pop", ignore_case = TRUE)))
sources <- popdf %>%
group_by(Songwriter) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(Name, Songwriter, total_words)
by_source_sentiment <- df_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, Name) %>%
ungroup() %>%
complete(sentiment, Name, fill = list(n = 0)) %>%
inner_join(sources, "Name") %>%
group_by(Songwriter, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
sentiment_diff <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words))) %>%
ungroup()
# plot
sentiment_diff %>%
mutate(sentiment = reorder(sentiment, estimate)) %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
rapdf <- unique_df_words %>%
filter(str_detect(Genre, regex("rap", ignore_case = TRUE)))
sources <- rapdf %>%
group_by(Songwriter) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(Name, Songwriter, total_words)
by_source_sentiment <- df_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, Name) %>%
ungroup() %>%
complete(sentiment, Name, fill = list(n = 0)) %>%
inner_join(sources, "Name") %>%
group_by(Songwriter, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
sentiment_diff <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words))) %>%
ungroup()
# plot
sentiment_diff %>%
mutate(sentiment = reorder(sentiment, estimate)) %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
word_counts <- df_words %>%
count(Name, word, sort = TRUE) %>%
filter(n > num_words_to_filter) %>%
ungroup() # do we need this?
word_count_matrix <- word_counts %>% cast_dtm(Name, word, n)
# create LDA object
songs_lda <- LDA(word_count_matrix, k = 2, control = list(seed = 0))
topics <- tidy(songs_lda, matrix = "beta")
# visualize
top_terms <- topics %>%
group_by(topic) %>%
top_n(40, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") + # partition by topic
coord_flip()
beta_spread <- topics %>%
mutate(topic = paste0("topic", topic)) %>% # adds topic to each number
spread(topic, beta) %>% # key, values w/ values for each var
filter(topic1 > .005 | topic2 > .005) %>%
mutate(log_ratio = log2(topic2 / topic1)) %>%
#filter(abs(log_ratio) < 50) %>%
arrange(-log_ratio)
# plot
beta_spread %>%
mutate(term = reorder(term, log_ratio)) %>%
ggplot(aes(term, log_ratio)) +
geom_col(show.legend = FALSE) +
coord_flip()
num_songs_present <- 10
# one entry for each song-word
each_song <- unique_df_words %>%
count(word, Name, Artists, Songwriter)
num_songs <- table(each_song$word)
# One entry for each artist-word
each_artist <- unique_df_words %>%
count(word, Artists)
num_artists <- table(each_artist$word)
good_words <- names(num_songs[num_songs > num_songs_present])
word_count_matrix <- each_song %>%
filter(word %in% good_words) %>%
cast_dtm(Name, word, n)
robotic <- each_song %>%
filter(word %in% good_words) %>%
distinct(Name, .keep_all = TRUE) %>%
select(Songwriter) %>%
unlist()
robotic <- robotic == "True"
pca <- prcomp(word_count_matrix)
# Plot first 2 PCs and authors
ggplot(, aes(x = pca$x[,1], y = pca$x[,2], col = as.factor(robotic))) +
geom_point() + xlab("First PC") +
ylab("Second PC") +
scale_colour_discrete(name = "Robotic")
km <- kmeans(pca$x[,1:2], centers = 2)
ggplot(, aes(x = pca$x[,1], y = pca$x[,2])) +
geom_point(aes(col = as.factor(robotic), pch = as.factor(km$cluster))) +
xlab("First PC") + ylab("Second PC") +
scale_colour_discrete(name = "Author") +
scale_shape_discrete(name = "Cluster") +
geom_point(aes(x = km$centers[,1], y = km$centers[,2], pch = factor(1:2)))
library(tidyr)
library(tidytext)
library(dplyr)
library(e1071)
library(tm)
library(topicmodels)
library(broom)
library(caret)
library(magrittr)
require(randomForest)
library = "C:\\Users\\liblabs-user\\Desktop\\song-authorship\\data"
laptop = "not yet"
desktop = "C:\\Users\\Sam\\Desktop\\song authorship\\data"
df <- read.csv(
paste(desktop, "\\Weekly_data_top_week.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
unique_df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized_unique.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
set.seed(1)
num_songs_present <- 5
num_words_to_filter <- 5
# one entry for each song-word
each_song <- unique_df_words %>%
count(word, ID, Artists, Songwriter)
head(unique_df_words)
library = "C:\\Users\\liblabs-user\\Desktop\\song-authorship\\data"
laptop = "not yet"
desktop = "C:\\Users\\Sam\\Desktop\\song authorship\\data"
setwd(desktop)
df <- read.csv(
paste(desktop, "\\Weekly_data_clean.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
df[is.na(df$Weeks.on.chart),"Weeks.on.chart"] <- 1
max_week <- df %>% group_by(Artists, Name) %>%
summarise(Weeks.on.chart = max(Weeks.on.chart, na.rm = TRUE))
#max_week[mapply(is.infinite, max_week)] <- 1
final_df <- merge(x = max_week, y = df, by = c("Artists", "Name", "Weeks.on.chart"), all.x = TRUE)
final_df$ID <- seq(1:nrow(final_df))
write.csv(final_df, file = "Weekly_data_top_week.csv")
head(final_df)
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df_words <- final_df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
#df_words$word <- text_tokens(df_words$word, stemmer = "en")
unique_df_words <- df_words %>% distinct(Name, Artists, word, .keep_all = TRUE)
write.csv(df_words, file = "Weekly_data_tokenized.csv")
write.csv(unique_df_words, file = "Weekly_data_tokenized_unique.csv")
library(tidyr)
library(tidytext)
library(dplyr)
library(e1071)
library(tm)
library(topicmodels)
library(broom)
library(caret)
library(magrittr)
require(randomForest)
library = "C:\\Users\\liblabs-user\\Desktop\\song-authorship\\data"
laptop = "not yet"
desktop = "C:\\Users\\Sam\\Desktop\\song authorship\\data"
df <- read.csv(
paste(desktop, "\\Weekly_data_top_week.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
unique_df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized_unique.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
set.seed(1)
num_songs_present <- 5
num_words_to_filter <- 5
# one entry for each song-word
each_song <- unique_df_words %>%
count(word, ID, Artists, Songwriter)
num_songs <- table(each_song$word)
good_words <- names(num_songs[num_songs > num_songs_present])
word_count_matrix <- df_words %>%
count(ID, word, sort = TRUE) %>%
filter(word %in% good_words, n > num_words_to_filter) %>%
ungroup() %>% # do we need this?
cast_dtm(ID, word, n)
#word_count_matrix <- #word_count_matrix[sample(nrow(word_count_matrix)),]
indices <- sample(word_count_matrix$dimnames$Docs)
dtm.train <- word_count_matrix[indices[1:(nrow(word_count_matrix)*0.8)],]
dtm.test <- word_count_matrix[indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)],]
df.train <- df[df$ID %in% indices[1:(nrow(word_count_matrix)*0.8)], ]
df.test <- df[df$ID %in% indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)], ]
df.train.lab <- as.factor(df.train$Songwriter)
df.test.lab <- as.factor(df.test$Songwriter)
convert_count <- function(x) {
y <- ifelse(x > 0, "Yes", "No")
y
}
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)
classifier <- naiveBayes(trainNB, df.train.lab)
pred <- predict(classifier, testNB)
conf.mat <- confusionMatrix(pred, df.test.lab)
conf.mat
forest <- train(x = as.matrix(dtm.train),
y = df.train.lab,
method = "rf",
ntree = 1,
trControl = trainControl(method = "oob"))
forest
predict(forest, df.test.lab)
forest$pred
forest.predict
predict(forest)
forest
forest$bestTune
forest$levels
forest$method
forest$modelInfo
forest$modelType
forest$results
forest$bestTune
pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
predict(forest, newdata = as.matrix(dtm.test))
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
predict(forest, type = "prob")
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
predict(forest, as.matrix(dtm.test))
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
as.matrix(dtm.test)
as.matrix(dtm.test$dimnames)
as.matrix(dtm.test$dimnames$Docs)
df.train$ID
df.test$ID
predict(forest$finalModel, df.test.lab)
predict(forest$finalModel, as.matrix(dtm.test))
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
head(df.test.lab)
head(df.test)
predict(forest$finalModel, as.matrix(dtm.test)) == df$Songwriter
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
predict(forest$finalModel, as.matrix(dtm.test)) == df.test$Songwriter
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
rownamespredict(forest$finalModel, as.matrix(dtm.test)))
rownames(predict(forest$finalModel, as.matrix(dtm.test)))
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
pred <- predict(forest$finalModel, as.matrix(dtm.test))
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
df.train$S''
df.train$Songwriter
trainNB
dtm.train$dim$Docs
df.test.lab
df.test
indices <- sample(word_count_matrix$dimnames$Docs)
dtm.train <- word_count_matrix[indices[1:(nrow(word_count_matrix)*0.8)],]
dtm.test <- word_count_matrix[indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)],]
df.train <- df[df$ID %in% indices[1:(nrow(word_count_matrix)*0.8)], ]
df.test <- df[df$ID %in% indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)], ]
df.train.lab <- as.factor(df.train$Songwriter)
df.test.lab <- as.factor(df.test$Songwriter)
confusionMatrix(c(1,1,1,0),c(0,1,1,0))
pred <- predict(forest$finalModel, as.matrix(dtm.test))
conf.mat <- confusionMatrix(pred, df.test.lab)
#pred <- extractPrediction(forest, testX = as.matrix(dtm.test), testY = df.test.lab)
conf.mat
forest <- train(x = as.matrix(dtm.train),
y = df.train.lab,
method = "rf",
ntree = 5,
trControl = trainControl(method = "oob"))
pred <- predict(forest$finalModel, as.matrix(dtm.test))
conf.mat <- confusionMatrix(pred, df.test.lab)
conf.mat
bagging
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(purrr)
library(magrittr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(broom)
# read in data
library = "C:\\Users\\liblabs-user\\Desktop\\song-authorship\\data"
laptop = "not yet"
desktop = "C:\\Users\\Sam\\Desktop\\song authorship\\data"
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df <- read.csv(
paste(desktop, "\\Weekly_data_top_week.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
unique_df_words <- read.csv(
paste(desktop, "\\Weekly_data_tokenized_unique.csv", sep = ""),
encoding = "UTF-8",
stringsAsFactors = FALSE)
set.seed(1)
num_songs_present <- 5
num_words_to_filter <- 5
# one entry for each song-word
each_song <- unique_df_words %>%
count(word, ID, Artists, Songwriter)
num_songs <- table(each_song$word)
good_words <- names(num_songs[num_songs > num_songs_present])
word_count_matrix <- df_words %>%
count(ID, word, sort = TRUE) %>%
filter(word %in% good_words, n > num_words_to_filter) %>%
ungroup() %>% # do we need this?
cast_dtm(ID, word, n)
indices <- sample(word_count_matrix$dimnames$Docs)
dtm.train <- word_count_matrix[indices[1:(nrow(word_count_matrix)*0.8)],]
dtm.test <- word_count_matrix[indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)],]
df.train <- df[df$ID %in% indices[1:(nrow(word_count_matrix)*0.8)], ]
df.test <- df[df$ID %in% indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)], ]
df.train.lab <- as.factor(df.train$Songwriter)
df.test.lab <- as.factor(df.test$Songwriter)
convert_count <- function(x) {
y <- ifelse(x > 0, "Yes", "No")
y
}
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)
classifier <- naiveBayes(trainNB, df.train.lab)
library(caret)
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
convert_count <- function(x) {
y <- ifelse(x > 0, "Yes", "No")
y
}
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)
classifier <- naiveBayes(trainNB, df.train.lab)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(purrr)
library(magrittr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(broom)
library(e1071)
library(tm)
library(caret)
require(randomForest)
convert_count <- function(x) {
y <- ifelse(x > 0, "Yes", "No")
y
}
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)
classifier <- naiveBayes(trainNB, df.train.lab)
pred <- predict(classifier, testNB)
conf.mat <- confusionMatrix(pred, df.test.lab)
conf.mat$table
conf.mat$overall
conf.mat
head(df)
df.Songwriter == FALSE
df$Songwriter == FALSE
df$Songwriter == "False"
sum(df$Songwriter == "False")
length(df)
dim(df)
dim(df)[0]
dim(df)[1]
sum(df$Songwriter == "False") / dim(df)[1]
forest <- train(x = as.matrix(dtm.train),
y = df.train.lab,
method = "rf",
ntree = 5,
trControl = trainControl(method = "oob"))
forest <- train(x = as.matrix(dtm.train),
y = df.train.lab,
