#Seasonal AR
Phi1 = .7
p = 12 		#season length (period) is 12
for (i in 51:(n+100)){
yt[i] = Phi1*yt[i-p] + epsilon[i]
}
yt = ts(yt[101:(n+100)])
par(mfrow=c(3,1))
plot(yt)
acf(yt,lag.max=40)
pacf(yt,lag.max=40)
#Seasonal AR
Phi1 = -.7
p = 12 		#season length (period) is 12
for (i in 51:(n+100)){
yt[i] = Phi1*yt[i-p] + epsilon[i]
}
yt = ts(yt[101:(n+100)])
par(mfrow=c(3,1))
plot(yt)
acf(yt,lag.max=40)
n = 1000
epsilon = rnorm(n+100)
yt = rep(NA,n+100)
yt[0:50]=0
#Seasonal AR
Phi1 = -.7
p = 12 		#season length (period) is 12
for (i in 51:(n+100)){
yt[i] = Phi1*yt[i-p] + epsilon[i]
}
yt = ts(yt[101:(n+100)])
par(mfrow=c(3,1))
plot(yt)
acf(yt,lag.max=40)
pacf(yt,lag.max=40)
#Seasonal MA
yt = rep(NA,n+100)
yt[0:50]=0
Theta1 = .7
for(i in 51:(n+100)){
yt[i] = epsilon[i] + Theta1*epsilon[i-p]
}
yt = ts(yt[101:(n+100)])
par(mfrow=c(3,1))
plot(yt)
acf(yt,lag.max=40)
pacf(yt,lag.max=40)
#Seasonal MA
yt = rep(NA,n+100)
yt[0:50]=0
Theta1 = -.7
for(i in 51:(n+100)){
yt[i] = epsilon[i] + Theta1*epsilon[i-p]
}
yt = ts(yt[101:(n+100)])
par(mfrow=c(3,1))
plot(yt)
acf(yt,lag.max=40)
pacf(yt,lag.max=40)
airpass = ts(AirPassengers,start=c(1949,1),frequency=12)
fit.sarima.011.011 = arima(SLO.recent,
order=c(0,1,1),								#1st difference, ARMA(0,1)
seasonal = list(order=c(0,1,1),period=12)	#1st seasonal difference, Seasonal ARMA(0,1)
)
n = 1000
epsilon = rnorm(n+100)
yt = rep(NA,n+100)
yt[0:50]=0
#Seasonal AR
Phi1 = -.7
p = 12 		#season length (period) is 12
for (i in 51:(n+100)){
yt[i] = Phi1*yt[i-p] + epsilon[i]
}
yt = ts(yt[101:(n+100)])
plot(yt)
acf(yt,lag.max=40)
pacf(yt,lag.max=40)
par(mfrow=c(3,1))
Theta1 = -.7
for(i in 51:(n+100)){
yt[i] = epsilon[i] + Theta1*epsilon[i-p]
}
yt = ts(yt[101:(n+100)])
plot(yt)
#Seasonal MA
yt = rep(NA,n+100)
acf(yt,lag.max=40)
yt[0:50]=0
fit.sarima.011.011 = arima(SLO.recent,
order=c(0,1,1),								#1st difference, ARMA(0,1)
seasonal = list(order=c(0,1,1),period=12)	#1st seasonal difference, Seasonal ARMA(0,1)
)
fit.sarima.011.011
airpass = ts(AirPassengers,start=c(1949,1),frequency=12)
par(mfrow=c(3,1))
pacf(yt,lag.max=40)
#airline passenger data
airpass = ts(AirPassengers,start=c(1949,1),frequency=12)
log.passengers = ts(log10(airpass),freq=12)
log.diff = diff(log.passengers, lag = 12)
par(mfrow=c(3,1))
plot(log.diff)
acf(log.diff, lag.max = 500)
pacf(log.diff, lag.max = 60)
fit.sarima.011.011 = arima(log.diff,
order=c(0,1,1),								#1st difference, ARMA(0,1)
seasonal = list(order=c(0,1,1),period=12)	#1st seasonal difference, Seasonal ARMA(0,1)
)
fit.sarima.011.011
par(mfrow=c(2,1))
acf(fit.sarima.011.011$resid,lag.max=40)
pacf(fit.sarima.011.011$resid,lag.max=40)
Box.test(fit.sarima.011.011$resid,lag=24)
n = length(log.diff)
forecast = predict(fit.sarima.011.011,n.ahead=36)
par(mfrow=c(1,1))
plot(1:n,log.diff,type="l",
xlim=c(0,n+36),
ylim=c(min(c(log.diff,forecast$pred-2*forecast$se)),
max(c(log.diff,forecast$pred+2*forecast$se))))	#plot original series
lines(n+(1:36),forecast$pred,col="red")
lines(n+(1:36),forecast$pred+2*forecast$se,col="blue")
lines(n+(1:36),forecast$pred-2*forecast$se,col="blue")
install.packages("installr")
require(installr)
updateR()
df <- read.csv("..\\..\\data\\Weekly_data_clean.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
df[is.na(df$Weeks.on.chart),"Weeks.on.chart"] <- 1
df[is.na(df$Peak.position), "Peak.position"] <- df[is.na(df$Peak.position), "Weekly.rank"]
max_week <- df %>% group_by(Artists, Name) %>%
summarise(Weeks.on.chart = max(Weeks.on.chart, na.rm = TRUE))
library(magrittr)
df <- read.csv("..\\..\\data\\Weekly_data_clean.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
df[is.na(df$Weeks.on.chart),"Weeks.on.chart"] <- 1
df[is.na(df$Peak.position), "Peak.position"] <- df[is.na(df$Peak.position), "Weekly.rank"]
max_week <- df %>% group_by(Artists, Name) %>%
summarise(Weeks.on.chart = max(Weeks.on.chart, na.rm = TRUE))
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
df <- read.csv("..\\..\\data\\Weekly_data_clean.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
df[is.na(df$Weeks.on.chart),"Weeks.on.chart"] <- 1
df[is.na(df$Peak.position), "Peak.position"] <- df[is.na(df$Peak.position), "Weekly.rank"]
max_week <- df %>% group_by(Artists, Name) %>%
summarise(Weeks.on.chart = max(Weeks.on.chart, na.rm = TRUE))
final_df <- merge(x = max_week, y = df, by = c("Artists", "Name", "Weeks.on.chart"), all.x = TRUE)
final_df$ID <- seq(1:nrow(final_df))
write.csv(final_df, file = "..\\..\\data\\Weekly_data_top_week.csv")
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df_words <- final_df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE) %>%
mutate(
word = gsub("([A-z])\\1{2,}", '\\1\\1', word), # ayyyy -> ayy
word = gsub("([A-z])'[A-z]+", '\\1', word),
word = gsub("[^A-z]", '', word)
) %>%
mutate(
Artists = str_to_lower(gsub('([A-z])([A-Z][a-z])|([a-z])([A-Z])', '\\1, \\2', Artists)),
Writing.Credits = str_to_lower(Writing.Credits)
) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
library(tidytext)
install.packages("tidytext")
library(tidytext)
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df_words <- final_df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE) %>%
mutate(
word = gsub("([A-z])\\1{2,}", '\\1\\1', word), # ayyyy -> ayy
word = gsub("([A-z])'[A-z]+", '\\1', word),
word = gsub("[^A-z]", '', word)
) %>%
mutate(
Artists = str_to_lower(gsub('([A-z])([A-Z][a-z])|([a-z])([A-Z])', '\\1, \\2', Artists)),
Writing.Credits = str_to_lower(Writing.Credits)
) %>%
filter(!word %in% stop_words$word, str_detect(word, "[a-z]"))
unique_df_words <- df_words %>% distinct(Name, Artists, word, .keep_all = TRUE)
write.csv(df_words, file = "..\\..\\data\\Weekly_data_tokenized.csv")
write.csv(unique_df_words, "..\\..\\data\\Weekly_data_tokenized_unique.csv")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
install.packages("tidyverse")
library(tidyverse)
library(tm)
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
install.packages("tidytext")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("tm")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
install.packages("tidytext")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("tm")
install.packages("tidytext")
install.packages("dplyr")
install.packages("topicmodels")
install.packages("broom")
install.packages("caret")
install.packages("dplyr")
install.packages("broom")
install.packages("randomForest")
install.packages("e1071")
install.packages("tidyverse")
library(tidyverse)
install.packages("broom")
install.packages("caret")
install.packages("broom")
install.packages("tidyverse")
install.packages("tidyverse")
require(tidyverse)
install.packages("dplyr")
install.packages("tidyverse")
install.packages("tidyverse")
require(tidyverse)
require(topicmodels)
require(SnowballC)
df <- read.csv("..\\..\\data\\Weekly_data_top_week.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df_words <- df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE)
df_words_unique <- df_words %>%
group_by(Artists, Name, Peak.position, Weeks.on.chart, Date, Genre, Writing.Credits, Songwriter) %>%
count(word) %>%
ungroup() %>%
group_by(Artists, Name, word, n, Songwriter, Writing.Credits, Genre) %>%
summarize_at(vars(Peak.position, Weeks.on.chart), funs(min(., na.rm = TRUE), max(., na.rm = TRUE))) %>% ungroup() %>%
select(-Weeks.on.chart_min, -Peak.position_max)
df_words <- df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE)
require(tidytext)
df_words <- df %>%
unnest_tokens(word, Lyrics, token = "regex", pattern = reg, collapse = TRUE)
df_words_unique <- df_words %>%
group_by(Artists, Name, Peak.position, Weeks.on.chart, Date, Genre, Writing.Credits, Songwriter) %>%
count(word) %>%
ungroup() %>%
group_by(Artists, Name, word, n, Songwriter, Writing.Credits, Genre) %>%
summarize_at(vars(Peak.position, Weeks.on.chart), funs(min(., na.rm = TRUE), max(., na.rm = TRUE))) %>% ungroup() %>%
select(-Weeks.on.chart_min, -Peak.position_max)
load(system.file("words", "english.RData", package = "SnowballC"))
each_song <- df_words_unique %>% mutate(
word = gsub("([A-z])\\1{2,}", '\\1\\1', word),
word = gsub("([A-z])'[A-z]+", '\\1', word),
word = gsub("[^A-z]", '', word),
word = gsub("\\s+", "", word)
) %>%
mutate(
Artists = str_to_lower(gsub('([A-z])([A-Z][a-z])|([a-z])([A-Z])', '\\1, \\2', Artists)),
Writing.Credits = str_to_lower(Writing.Credits),
Contributors = str_split(str_to_lower(paste(Artists, Writing.Credits, sep = ", ")), ", ")
)
each_song$Contributors = sapply(each_song$Contributors, function(x) toString(unique(unlist(x))))
each_song$Contributors = gsub("\n", "", each_song$Contributors, fixed = TRUE)
```{r}
all_words <- unique(each_song$word)
all_words <- unique(each_song$word)
num_songs <- table(each_song$word)
word_rarity <- -log(num_songs/max(num_songs))
word_length <- sapply(all_words, str_length)
each_song_once <- each_song %>% mutate(
rarity = word_rarity[word],
word_len = word_length[word]
) %>% na.omit() %>%
group_by(Name, Artists, Songwriter) %>%
summarise_at(vars(rarity, word_len), funs(max) )
unique_words <- each_song %>% group_by(Name, Artists) %>% count()
idf <- log(nrow(df) / num_songs)
tf_idf_score <- each_song %>%
mutate(wordtfidf = as.numeric(n * idf[word])) %>%
group_by(Artists, Name) %>%
summarize_at(vars(wordtfidf), sum, na.rm = TRUE) %>%
left_join(unique_words) %>%
mutate(
tf_idf_complexity = wordtfidf / nn
)
tf_idf_score
song_complexity <- each_song %>%  mutate(
word_length = str_length(word),
word_rare = word_rarity[word],
is_profanity = word %in% c("fuck", "shit", "bitch")
) %>% group_by(Name, Artists, Songwriter) %>%
summarize(
Total_Words = sum(n),
Unique_Words = length(n),
diversity = Unique_Words/Total_Words,
mean_word_rarity = mean(word_rare, na.rm = TRUE),
median_word_rarity = median(word_rare, na.rm = TRUE),
mean_word_length = mean(word_length, na.rm = TRUE),
median_word_length = median(word_length, na.rm = TRUE),
num_profanity = sum(is_profanity)
) %>%
left_join(tf_idf_score)
write.csv(song_complexity, paste0(desktop, "//Song_Complexity.csv"))
write.csv(song_complexity, "..\\..\\data\\Song_Complexity.csv"))
write.csv(song_complexity, "..\\..\\data\\Song_Complexity.csv")
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#install.packages("tidytext")
#install.packages("dplyr")
#install.packages("ggplot2")
install.packages("tm")
#install.packages("topicmodels")
#install.packages("broom")
install.packages("caret")
#install.packages("broom")
install.packages("randomForest")
install.packages("e1071")
library(purrr)
library(magrittr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(broom)
library(e1071)
library(caret)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
#install.packages("tidytext")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("tm")
#install.packages("topicmodels")
#install.packages("broom")
#install.packages("caret")
#install.packages("broom")
#install.packages("randomForest")
#install.packages("e1071")
library(purrr)
library(magrittr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(broom)
library(e1071)
library(caret)
library(randomForest)
# read in data
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df <- read.csv(
"..\\..\\data\\Weekly_data_top_week.csv", sep = "",
encoding = "UTF-8",
stringsAsFactors = FALSE)
# read in data
reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"
df <- read.csv(
"..\\..\\data\\Weekly_data_top_week.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
df_words <- read.csv(
"..\\..\\data\\Weekly_data_tokenized.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
unique_df_words <- read.csv(
"..\\..\\data\\Weekly_data_tokenized_unique.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE)
metrics <- read.csv(
"..\\..\\data\\Song_Complexity.csv",
encoding = "UTF-8",
stringsAsFactors = FALSE
)
df <- merge(x = df, y = metrics, by = c("X"), all.x = TRUE)
colnames(df)[colnames(df) == "Songwriter.x"] <- "Songwriter"
#final_df <- merge(x = max_week, y = df, by = c("Artists", "Name", "Weeks.on.chart"), all.x = TRUE)
set.seed(1)
num_songs_present <- 5
num_words_to_filter <- 5
# one entry for each song-word
each_song <- unique_df_words %>%
count(word, ID, Artists, Songwriter)
num_songs <- table(each_song$word)
good_words <- names(num_songs[num_songs > num_songs_present])
word_count_matrix <- df_words %>%
count(ID, word, sort = TRUE) %>%
filter(word %in% good_words, n > num_words_to_filter) %>%
ungroup() %>% # do we need this?
cast_dtm(ID, word, n)
indices <- sample(word_count_matrix$dimnames$Docs)
dtm.train <- word_count_matrix[indices[1:(nrow(word_count_matrix)*0.8)],]
dtm.test <- word_count_matrix[indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)],]
df.train <- df[df$ID %in% indices[1:(nrow(word_count_matrix)*0.8)], ]
df.test <- df[df$ID %in% indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)], ]
df.train.lab <- as.factor(df.train$Songwriter)
df.test.lab <- as.factor(df.test$Songwriter)
convert_count <- function(x) {
y <- ifelse(x > 0, "Yes", "No")
y
}
trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)
testNB
classifier <- naiveBayes(trainNB, df.train.lab)
pred <- predict(classifier, testNB)
conf.mat <- confusionMatrix(pred, df.test.lab)
conf.mat
sum(df$Songwriter == "False") / dim(df)[1]
forest <- train(x = as.matrix(dtm.train),
y = df.train.lab,
method = "rf",
ntree = 5,
trControl = trainControl(method = "oob"))
pred <- predict(forest$finalModel, as.matrix(dtm.test))
conf.mat <- confusionMatrix(pred, df.test.lab)
conf.mat
df$Songwriter <- factor(df$Songwriter)
logit <- glm(Songwriter ~ Peak.position + Weeks.on.chart + diversity + num_profanity + tf_idf_complexity,
data = df,
family = "binomial")
summary(logit)
num_words_to_filter = 5
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
sources <- unique_df_words %>%
group_by(Songwriter) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(Name, Songwriter, total_words)
by_source_sentiment <- unique_df_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, Name) %>%
ungroup() %>%
complete(sentiment, Name, fill = list(n = 0)) %>%
inner_join(sources, "Name") %>%
group_by(Songwriter, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
sentiment_diff <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words))) %>%
ungroup() %>%
mutate(sentiment = reorder(sentiment, estimate),
subset = "All genres")
# plot
sentiment_diff  %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
num_words_to_filter = 5
nrc <- sentiments %>%
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment)
sources <- unique_df_words %>%
group_by(Songwriter) %>%
mutate(total_words = n()) %>%
ungroup() %>%
distinct(Name, Songwriter, total_words)
by_source_sentiment <- unique_df_words %>%
inner_join(nrc, by = "word") %>%
count(sentiment, Name) %>%
ungroup() %>%
complete(sentiment, Name, fill = list(n = 0)) %>%
inner_join(sources, "Name") %>%
group_by(Songwriter, sentiment, total_words) %>%
summarize(words = sum(n)) %>%
ungroup()
sentiment_diff <- by_source_sentiment %>%
group_by(sentiment) %>%
do(tidy(poisson.test(.$words, .$total_words))) %>%
ungroup() %>%
mutate(sentiment = reorder(sentiment, estimate),
subset = "All genres")
# plot
sentiment_diff  %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
# plot
sentiment_diff  %>%
ggplot(aes(sentiment, estimate)) +
geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
geom_point(shape = 21, size = 2.5, fill = "white") +
scale_y_continuous("% change in Robotic relative to non-Robotic",
breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
coord_flip()
unique_df_words
head(unique_df_words)
unique_df_words$Genre
df_words$Genre
unique(df_words$Genre)
