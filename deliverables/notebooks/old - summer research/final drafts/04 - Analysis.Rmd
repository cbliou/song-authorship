---
title: "04 - Analysis"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

#install.packages("tidytext")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("tm")
#install.packages("topicmodels")
#install.packages("broom")
#install.packages("caret")
#install.packages("broom")
#install.packages("randomForest")
#install.packages("e1071")

library(purrr)
library(magrittr)
library(tidyr)
library(tidytext)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(broom)

library(e1071)
library(caret)
library(randomForest)
```

```{r, include=FALSE}

# read in data

reg <- "([^A-Za-z\\d#@']+|'[^A-Za-z\\d#@]+)"

df <- read.csv(
  "..\\..\\data\\Weekly_data_top_week.csv",
  encoding = "UTF-8",
  stringsAsFactors = FALSE)

df_words <- read.csv(
  "..\\..\\data\\Weekly_data_tokenized.csv", 
  encoding = "UTF-8",
  stringsAsFactors = FALSE)

unique_df_words <- read.csv(
  "..\\..\\data\\Weekly_data_tokenized_unique.csv", 
  encoding = "UTF-8",
  stringsAsFactors = FALSE)

metrics <- read.csv(
  "..\\..\\data\\Song_Complexity.csv",
  encoding = "UTF-8",
  stringsAsFactors = FALSE
)

df <- merge(x = df, y = metrics, by = c("X"), all.x = TRUE)
colnames(df)[colnames(df) == "Songwriter.x"] <- "Songwriter"

#final_df <- merge(x = max_week, y = df, by = c("Artists", "Name", "Weeks.on.chart"), all.x = TRUE)

```

# Introduction
The Billboard Charts are an industry standard of musical success. Nowadays, a non-trivial number of songs on the charts are co-authored by songwriters (footnote here to describe what a songwriter is). We seek to compare songwritten songs with non-songwritten songs and to determine the existence of linguistic differences between to the two types.

# Research Questions

We first define a song as "Robotic" if the song is co-written by one of the songwriters we identify as common songwriters This is merely semantic and allows us to say "robotic" instead of "songs written by songwriters."  

Our primary research question includes determining linguistic features of lyrics that best differentiate Robotic songs from original songs. Our second research question involves finding differences between Robotic and original songs in general, including lyrical complexity and ranking data.


# Approach

## Data collection
To obtain song names and chart data, we scraped the previous 500 weeks of the Billboard Hot 100 chart. These songs were passed through the Genius API to obtain lyrics, writing credits, and genre. LA Weekly published [an article](https://www.laweekly.com/music/the-20-best-pop-songwriters-in-2017-8763662)  that contained 20 prolific songwriters in 2017. We used this list of songwriters as a starting point and eventually added more songwriters by hand.

Stop words were removed in the exploratory and predictive analysis.

## Methods

```{r, include=FALSE}
set.seed(1)
num_songs_present <- 5
num_words_to_filter <- 5

# one entry for each song-word
each_song <- unique_df_words %>%
    count(word, ID, Artists, Songwriter)

num_songs <- table(each_song$word)

good_words <- names(num_songs[num_songs > num_songs_present])

word_count_matrix <- df_words %>%
  count(ID, word, sort = TRUE) %>%
  filter(word %in% good_words, n > num_words_to_filter) %>%
  ungroup() %>% # do we need this?
  cast_dtm(ID, word, n)

indices <- sample(word_count_matrix$dimnames$Docs)

dtm.train <- word_count_matrix[indices[1:(nrow(word_count_matrix)*0.8)],]
dtm.test <- word_count_matrix[indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)],]

df.train <- df[df$ID %in% indices[1:(nrow(word_count_matrix)*0.8)], ]
df.test <- df[df$ID %in% indices[(nrow(word_count_matrix)*0.8+1):nrow(word_count_matrix)], ]
df.train.lab <- as.factor(df.train$Songwriter)
df.test.lab <- as.factor(df.test$Songwriter)
```

We utilized several classification algorithms to determine if there exist features that differentiatiate robotic and non-robotic songs. We start with purely lyrical content and end with chart data.

### Naive Bayes

The first model is a modified version of Naive Bayes. Instead of collecting word frequencies, modified Naive Bayes only checks whether a word occurs in a song or not. This was run on a term-document matrix of the lyrics.


```{r}
convert_count <- function(x) {
  y <- ifelse(x > 0, "Yes", "No")
  y
}

trainNB <- apply(dtm.train, MARGIN = 2, convert_count)
testNB <- apply(dtm.test, MARGIN = 2, convert_count)

classifier <- naiveBayes(trainNB, df.train.lab)
pred <- predict(classifier, testNB)
conf.mat <- confusionMatrix(pred, df.test.lab)

conf.mat
```

We were able to achieve a 78.46% accuracy rate on the validation set. This is much better than randomly guessing whether a song was robotic or not. We would like to point out our data suffers from a large class imbalance: around 89% of songs was not co-written with a songwriter. Although Naive Bayes was able to achieve an accuracy rate that is better than randomly guessing, it was not able to outperform a simple classifier that predicts "non-Robotic" on every validation sample. Moreover, the specificity rate tells us only 3% of robotic songs were classified correctly.

```{r, include = FALSE}
sum(df$Songwriter == "False") / dim(df)[1]
```

### Random Forest

We also deployed a random forest on the term-document matrix.

```{r}
forest <- train(x = as.matrix(dtm.train),
                y = df.train.lab,
                method = "rf",
                ntree = 5,
                trControl = trainControl(method = "oob"))
pred <- predict(forest$finalModel, as.matrix(dtm.test))
conf.mat <- confusionMatrix(pred, df.test.lab)

conf.mat
```

Our overall accuracy rate of __ was higher than Naive Bayes. However, the random forest predicted every song to be non-robotic. This likely happened due to the class imbalance in our data.

### LSTM neural network

The last classification algorithm we deployed was a LSTM neural network. If the reader is unfamiliar with LSTM networks, a great introduction is available at [colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). We were only able to achieve an accuracy of 60% on the validation set. However, the specificity rate of 11.2% shows the neural network is able to classify robotic songs better than the previous algorithms.

Overall, our classification models show that there is a difference in the linguistic usage of robotic vs. non-robotic songs.

### Logistic regression

We utilized logisitic regression on the peak position of a song on the Billboard charts and the number of weeks on the chart to predict if a song is robotic or not.

```{r}
df$Songwriter <- factor(df$Songwriter)
logit <- glm(Songwriter ~ Peak.position + Weeks.on.chart + diversity + num_profanity + tf_idf_complexity, 
             data = df,
             family = "binomial")
summary(logit)
```

Key takeaways:

- For every one unit increase in the peak position, the log odds of Robotic vs. non-Robotic decreases by 0.019
 - This means songs that are ranked lower on Billboard are more likely to be non-robotic.
- For every one unit increase in the number of weeks on the Hot 100, the log odds of a song being Robotic increaes by 0.0011.
 - This means songs that chart for longer on Billboard are more likely to be Robotic.

```{r, include = FALSE}
#logit <- glm(Songwriter ~ Peak.position,
#             data = df,
#             family = "binomial")
#newdat <- data.frame(Peak.position=seq(min(df$Peak.position), max(df$Peak.position)))
#newdat$pred = predict(logit, newdat, type = "response")
#plot(as.numeric(Songwriter) - 1 ~ Peak.position, data=df, col="red4")
#lines(pred ~ Peak.position, newdat, col="green4", lwd=2)
```


## Exploratory

### Sentiment Analysis

For sentiments, we used the (NRC Emotion Lexicon)[https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm]. We run a Poisson test on each sentiment between robotic and non-robotic songs. The 95% confidence intervals are plotted below:

```{r, include = FALSE, fig.width = 8, fig.height = 6}
num_words_to_filter = 5

nrc <- sentiments %>%
    filter(lexicon == "nrc") %>%
    dplyr::select(word, sentiment)

sources <- unique_df_words %>%
    group_by(Songwriter) %>%
    mutate(total_words = n()) %>%
    ungroup() %>%
    distinct(Name, Songwriter, total_words)

by_source_sentiment <- unique_df_words %>%
    inner_join(nrc, by = "word") %>%
    count(sentiment, Name) %>%
    ungroup() %>%
    complete(sentiment, Name, fill = list(n = 0)) %>%
    inner_join(sources, "Name") %>%
    group_by(Songwriter, sentiment, total_words) %>%
    summarize(words = sum(n)) %>%
    ungroup()

sentiment_diff <- by_source_sentiment %>%
    group_by(sentiment) %>%
    do(tidy(poisson.test(.$words, .$total_words))) %>%
    ungroup() %>%
    mutate(sentiment = reorder(sentiment, estimate),
           subset = "All genres")

# plot
sentiment_diff  %>%
    ggplot(aes(sentiment, estimate)) + 
    geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
    geom_point(shape = 21, size = 2.5, fill = "white") +
    scale_y_continuous("% change in Robotic relative to non-Robotic",
                       breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
                       labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
    coord_flip()
```

Key takeaways:

In general, non-Robotic songs utilize less words with sentiments relating to joy, positivity, sadness, negativity, and fear than robotic songs. 

The sentiment trend described above holds for the Pop genre:

```{r, include = FALSE, fig.width = 8, fig.height = 6}
popdf <- unique_df_words %>% 
    filter(str_detect(Genre, regex("pop", ignore_case = TRUE)))

sources <- popdf %>%
    group_by(Songwriter) %>%
    mutate(total_words = n()) %>%
    ungroup() %>%
    distinct(Name, Songwriter, total_words)

by_source_sentiment <- unique_df_words %>%
    inner_join(nrc, by = "word") %>%
    count(sentiment, Name) %>%
    ungroup() %>%
    complete(sentiment, Name, fill = list(n = 0)) %>%
    inner_join(sources, "Name") %>%
    group_by(Songwriter, sentiment, total_words) %>%
    summarize(words = sum(n)) %>%
    ungroup()

sentiment_diff_pop <- by_source_sentiment %>%
    group_by(sentiment) %>%
    do(tidy(poisson.test(.$words, .$total_words))) %>%
    ungroup()%>%
    mutate(sentiment = reorder(sentiment, estimate),
           subset = "Pop")

# plot
sentiment_diff_pop  %>%
    ggplot(aes(sentiment, estimate)) + 
    geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
    geom_point(shape = 21, size = 2.5, fill = "white") +
    scale_y_continuous("% change in Robotic relative to non-Robotic",
                       breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
                       labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
    coord_flip()
```

and the rap genre:

```{r, include = FALSE, fig.width = 8, fig.height = 6}
rapdf <- unique_df_words %>% 
    filter(str_detect(Genre, regex("rap", ignore_case = TRUE)))

sources <- rapdf %>%
    group_by(Songwriter) %>%
    mutate(total_words = n()) %>%
    ungroup() %>%
    distinct(Name, Songwriter, total_words)

by_source_sentiment <- unique_df_words %>%
    inner_join(nrc, by = "word") %>%
    count(sentiment, Name) %>%
    ungroup() %>%
    complete(sentiment, Name, fill = list(n = 0)) %>%
    inner_join(sources, "Name") %>%
    group_by(Songwriter, sentiment, total_words) %>%
    summarize(words = sum(n)) %>%
    ungroup()

sentiment_diff_rap <- by_source_sentiment %>%
    group_by(sentiment) %>%
    do(tidy(poisson.test(.$words, .$total_words))) %>%
    ungroup() %>%
    mutate(sentiment = reorder(sentiment, estimate),
           subset = "Rap")

# plot
sentiment_diff_rap  %>%
    ggplot(aes(sentiment, estimate)) + 
    geom_errorbar(width = .5, aes(ymin = conf.low, ymax = conf.high)) +
    geom_point(shape = 21, size = 2.5, fill = "white") +
    scale_y_continuous("% change in Robotic relative to non-Robotic",
                       breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
                       labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
    coord_flip()
```

```{r}
rockdf <- unique_df_words %>% 
    filter(str_detect(Genre, regex("rock", ignore_case = TRUE)))

sources <- rockdf %>%
    group_by(Songwriter) %>%
    mutate(total_words = n()) %>%
    ungroup() %>%
    distinct(Name, Songwriter, total_words)

by_source_sentiment <- unique_df_words %>%
    inner_join(nrc, by = "word") %>%
    count(sentiment, Name) %>%
    ungroup() %>%
    complete(sentiment, Name, fill = list(n = 0)) %>%
    inner_join(sources, "Name") %>%
    group_by(Songwriter, sentiment, total_words) %>%
    summarize(words = sum(n)) %>%
    ungroup()

sentiment_diff_rock <- by_source_sentiment %>%
    group_by(sentiment) %>%
    do(tidy(poisson.test(.$words, .$total_words))) %>%
    ungroup()%>%
    mutate(sentiment = reorder(sentiment, estimate),
           subset = "Rock")



all_sents <- rbind(sentiment_diff,
                   sentiment_diff_pop,
                   sentiment_diff_rap,
                   sentiment_diff_rock)

all_sents %>%
  ggplot(aes(sentiment, estimate, colour = subset)) +
  scale_color_manual(values=c("#154D33", "#E69F00", "#56B4E9", "#ba2525")) +
  geom_point(position = position_dodge(width = 0.5)) + 
  geom_errorbar(width = 0.5, aes(ymin = conf.low, ymax = conf.high), position = "dodge") +
  scale_y_continuous("% change in Robotic relative to non-Robotic",
                     breaks = c(0.6, 0.8, 1, 1.2, 1.4, 1.6),
                     labels = c("-40%", "-20%", "0%", "20%", "40%", "60%")) +
  coord_flip()

ggsave("sentiment.png", width = 25, height = 19.5, units = "cm")

```


This shows that sentiment differences are nearly uniform across the common genres on Billboard. As a result, we can conclude there are significiant sentiment differences between robotic and non-robotic songs.


## Latent Dirichlet Allocation

To further explore our dataset for hidden structure, we applied Latent Dirichlet Allocation. We start with two topics. Below is a visualization of words that are most likely to be generated from the respective topic (terms with highest $\beta$):

```{r, fig.width = 8, fig.height = 9, echo = FALSE}
word_counts <- df_words %>%
    count(Name, word, sort = TRUE) %>%
    filter(n > num_words_to_filter) %>%
    ungroup() # do we need this?
    
word_count_matrix <- word_counts %>% cast_dtm(Name, word, n)

# create LDA object
songs_lda <- LDA(word_count_matrix, k = 2, control = list(seed = 0))
topics <- tidy(songs_lda, matrix = "beta")
    
# visualize
top_terms <- topics %>%
    group_by(topic) %>%
    top_n(40, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)
    
top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") + # partition by topic
    coord_flip()
```

Key takeaways:

- Words most likely to be generated from topic 1 include love, yeah, baby, wanna, la, and ooh. These seem to originate from pop songs.
- In contrast, words most likely to be generated from topic 2 include many expletives, particularly ones in rap songs. 

This leads us to believe LDA accurately captures a difference between the language used in pop songs and rap songs.

*confused on how this fits into robotic v non-robotic story*

# Limitations

## From Genius

We may have picked up lyrics of covers and remixes instead of the original lyrics. 

## Lyrical ranking

Define justifiable metric(s) for "lyrical complexity"

Do this today!

# Future work

- Would like to utilize musical markers instead of just lyrics
- Would like to delve into the change of artists' success over time
